/**
 * File Chunking Service
 *
 * Orchestrates the chunking of extracted text from files:
 * - Reads extracted_text from files with processing_status='completed'
 * - Applies appropriate chunking strategy based on MIME type
 * - Inserts chunks into file_chunks table
 * - Enqueues EmbeddingGenerationJob for vector indexing
 * - Updates embedding_status to 'queued'
 *
 * This service bridges the gap between text extraction (FileProcessingService)
 * and embedding generation (EmbeddingService via MessageQueue).
 *
 * @module services/files/FileChunkingService
 */

import { v4 as uuidv4 } from 'uuid';
import { executeQuery, SqlParams } from '@/infrastructure/database/database';
import { createChildLogger } from '@/shared/utils/logger';
import { ChunkingStrategyFactory } from '../chunking/ChunkingStrategyFactory';
import type { ChunkingOptions } from '../chunking/types';
import type { FileChunkingJob, EmbeddingGenerationJob } from '@/infrastructure/queue/MessageQueue';

// Child logger for this service (uses createChildLogger for LOG_SERVICES filtering)
const logger = createChildLogger({ service: 'FileChunkingService' });

/**
 * Default chunking options
 * - 512 tokens max per chunk (optimal for embeddings)
 * - 50 token overlap for context continuity
 */
const DEFAULT_CHUNKING_OPTIONS: ChunkingOptions = {
  maxTokens: 512,
  overlapTokens: 50,
};

/**
 * Image MIME types that don't require text chunking
 * Images are handled differently - embeddings generated via Azure Computer Vision
 */
const IMAGE_MIME_TYPES = new Set([
  'image/jpeg',
  'image/png',
  'image/gif',
  'image/webp',
]);

/**
 * Result of chunking operation
 */
export interface ChunkingResult {
  fileId: string;
  chunkCount: number;
  totalTokens: number;
  embeddingJobId?: string;
}

/**
 * File data needed for chunking
 */
interface FileForChunking {
  id: string;
  userId: string;
  mimeType: string;
  extractedText: string | null;
  processingStatus: string;
  embeddingStatus: string;
}

/**
 * File Chunking Service Class
 *
 * Singleton service that chunks extracted text and prepares for embedding.
 */
export class FileChunkingService {
  private static instance: FileChunkingService | null = null;

  private constructor() {
    logger.info('FileChunkingService initialized');
  }

  /**
   * Get singleton instance
   */
  public static getInstance(): FileChunkingService {
    if (!FileChunkingService.instance) {
      FileChunkingService.instance = new FileChunkingService();
    }
    return FileChunkingService.instance;
  }

  /**
   * Reset singleton (for testing)
   */
  public static resetInstance(): void {
    FileChunkingService.instance = null;
  }

  /**
   * Process file chunks
   *
   * Main entry point called by MessageQueue worker.
   *
   * @param jobData - File chunking job data
   * @returns Chunking result
   */
  public async processFileChunks(jobData: FileChunkingJob): Promise<ChunkingResult> {
    const { fileId, userId, mimeType, sessionId } = jobData;

    logger.info({ fileId, userId, mimeType }, 'Starting file chunking');

    // Handle images separately - they don't have text to chunk
    // Image embeddings are generated by ImageProcessor and persisted by FileProcessingService
    // We need to index them in Azure AI Search for visual search
    if (IMAGE_MIME_TYPES.has(mimeType)) {
      logger.info(
        { fileId, userId, mimeType },
        'Handling image file - indexing embedding in Azure AI Search'
      );

      await this.indexImageEmbedding(fileId, userId, sessionId);

      return {
        fileId,
        chunkCount: 0,
        totalTokens: 0,
      };
    }

    // 1. Get file with extracted_text
    const file = await this.getFileForChunking(fileId, userId);

    if (!file) {
      throw new Error(`File not found: ${fileId}`);
    }

    if (!file.extractedText) {
      logger.warn({ fileId }, 'No extracted text found, skipping chunking');
      return {
        fileId,
        chunkCount: 0,
        totalTokens: 0,
      };
    }

    // 2. Validate processing status
    if (file.processingStatus !== 'completed') {
      throw new Error(`File processing not completed: ${file.processingStatus}`);
    }

    // 3. Update embedding status to 'processing'
    await this.updateEmbeddingStatus(fileId, 'processing');

    try {
      // 4. Select chunking strategy based on MIME type
      const strategy = ChunkingStrategyFactory.createForFileType(mimeType, DEFAULT_CHUNKING_OPTIONS);

      // 5. Generate chunks
      const chunks = strategy.chunk(file.extractedText);

      logger.info({ fileId, chunkCount: chunks.length }, 'Generated chunks');

      // 6. Insert chunks into database
      const chunkRecords = await this.insertChunks(fileId, userId, chunks);

      // 7. Calculate total tokens
      const totalTokens = chunks.reduce((sum, chunk) => sum + chunk.tokenCount, 0);

      // 8. Enqueue embedding generation job
      const embeddingJobId = await this.enqueueEmbeddingJob(fileId, userId, chunkRecords, mimeType);

      // 9. Update embedding status to 'queued'
      await this.updateEmbeddingStatus(fileId, 'queued');

      logger.info({
        fileId,
        chunkCount: chunks.length,
        totalTokens,
        embeddingJobId,
      }, 'File chunking completed successfully');

      return {
        fileId,
        chunkCount: chunks.length,
        totalTokens,
        embeddingJobId,
      };
    } catch (error) {
      // Revert status on failure
      await this.updateEmbeddingStatus(fileId, 'failed');
      throw error;
    }
  }

  /**
   * Get file data for chunking
   *
   * @param fileId - File ID
   * @param userId - User ID (for multi-tenant isolation)
   * @returns File data or null if not found
   */
  private async getFileForChunking(fileId: string, userId: string): Promise<FileForChunking | null> {
    const result = await executeQuery<{
      id: string;
      user_id: string;
      mime_type: string;
      extracted_text: string | null;
      processing_status: string;
      embedding_status: string;
    }>(
      `SELECT id, user_id, mime_type, extracted_text, processing_status, embedding_status
       FROM files
       WHERE id = @fileId AND user_id = @userId`,
      { fileId, userId }
    );

    const row = result.recordset[0];
    if (!row) {
      return null;
    }

    return {
      id: row.id,
      userId: row.user_id,
      mimeType: row.mime_type,
      extractedText: row.extracted_text,
      processingStatus: row.processing_status,
      embeddingStatus: row.embedding_status,
    };
  }

  /**
   * Update embedding status
   *
   * @param fileId - File ID
   * @param status - New embedding status
   */
  private async updateEmbeddingStatus(
    fileId: string,
    status: 'pending' | 'processing' | 'queued' | 'completed' | 'failed'
  ): Promise<void> {
    await executeQuery(
      `UPDATE files SET embedding_status = @status, updated_at = GETUTCDATE() WHERE id = @fileId`,
      { fileId, status }
    );

    logger.debug({ fileId, status }, 'Updated embedding status');
  }

  /**
   * Index image embedding in Azure AI Search
   *
   * Retrieves the embedding from ImageEmbeddingRepository and indexes it
   * in Azure AI Search for visual search capability. Also passes AI-generated caption for improved search relevance.
   * Emits WebSocket event to notify frontend of readiness state change.
   *
   * @param fileId - File ID
   * @param userId - User ID
   * @param sessionId - Session ID for WebSocket room targeting (optional)
   */
  private async indexImageEmbedding(fileId: string, userId: string, sessionId?: string): Promise<void> {
    try {
      // Mark as processing
      await this.updateEmbeddingStatus(fileId, 'processing');

      const { getImageEmbeddingRepository } = await import(
        '@/repositories/ImageEmbeddingRepository'
      );
      const repository = getImageEmbeddingRepository();
      const embeddingRecord = await repository.getByFileId(fileId, userId);

      if (!embeddingRecord) {
        logger.warn({ fileId, userId }, 'No image embedding found - marking as completed without indexing');
        await this.updateEmbeddingStatus(fileId, 'completed');

        // Emit readiness_changed event even when no embedding (file is still "ready")
        await this.emitReadinessChanged(fileId, userId, sessionId);
        return;
      }

      // Get file name and mime_type from files table
      const fileResult = await executeQuery<{ name: string; mime_type: string | null }>(
        `SELECT name, mime_type FROM files WHERE id = @fileId AND user_id = @userId`,
        { fileId, userId }
      );

      const fileName = fileResult.recordset[0]?.name || 'unknown.jpg';
      const fileMimeType = fileResult.recordset[0]?.mime_type ?? undefined;

      // Index in Azure AI Search
      const { VectorSearchService } = await import('@services/search/VectorSearchService');
      const vectorSearchService = VectorSearchService.getInstance();

      await vectorSearchService.indexImageEmbedding({
        fileId,
        userId,
        embedding: embeddingRecord.embedding,
        fileName,
        caption: embeddingRecord.caption ?? undefined,
        mimeType: fileMimeType,
      });

      // Mark as completed
      await this.updateEmbeddingStatus(fileId, 'completed');

      logger.info(
        {
          fileId,
          userId,
          dimensions: embeddingRecord.embedding.length,
          hasCaption: !!embeddingRecord.caption,
        },
        'Image embedding indexed in Azure AI Search successfully'
      );

      // Emit WebSocket event to notify frontend
      await this.emitReadinessChanged(fileId, userId, sessionId);
    } catch (error) {
      logger.error(
        {
          error: error instanceof Error ? error.message : String(error),
          fileId,
          userId,
        },
        'Failed to index image embedding'
      );

      await this.updateEmbeddingStatus(fileId, 'failed');
      throw error;
    }
  }

  /**
   * Emit readiness_changed WebSocket event
   *
   * Notifies frontend that file is ready for use (processing + embedding completed).
   *
   * @param fileId - File ID
   * @param userId - User ID
   * @param sessionId - Session ID for WebSocket room targeting
   */
  private async emitReadinessChanged(fileId: string, userId: string, sessionId?: string): Promise<void> {
    try {
      const { getFileEventEmitter } = await import('@/domains/files/emission/FileEventEmitter');
      const { FILE_READINESS_STATE, PROCESSING_STATUS, EMBEDDING_STATUS } = await import(
        '@bc-agent/shared'
      );

      const eventEmitter = getFileEventEmitter();

      eventEmitter.emitReadinessChanged(
        { fileId, userId, sessionId },
        {
          previousState: FILE_READINESS_STATE.PROCESSING,
          newState: FILE_READINESS_STATE.READY,
          processingStatus: PROCESSING_STATUS.COMPLETED,
          embeddingStatus: EMBEDDING_STATUS.COMPLETED,
        }
      );

      logger.debug({ fileId, userId, sessionId: !!sessionId }, 'Emitted readiness_changed event');
    } catch (error) {
      // Don't fail the operation if event emission fails
      logger.warn(
        {
          error: error instanceof Error ? error.message : String(error),
          fileId,
          userId,
        },
        'Failed to emit readiness_changed event - file processing still succeeded'
      );
    }
  }

  /**
   * Insert chunks into database
   *
   * @param fileId - File ID
   * @param userId - User ID
   * @param chunks - Chunks from chunking strategy
   * @returns Array of chunk records with IDs
   */
  private async insertChunks(
    fileId: string,
    userId: string,
    chunks: Array<{ text: string; chunkIndex: number; tokenCount: number; metadata?: Record<string, unknown> }>
  ): Promise<Array<{ id: string; text: string; chunkIndex: number; tokenCount: number }>> {
    const chunkRecords: Array<{ id: string; text: string; chunkIndex: number; tokenCount: number }> = [];

    for (const chunk of chunks) {
      // All IDs must be UPPERCASE per CLAUDE.md
      const chunkId = uuidv4().toUpperCase();

      // Table schema (after migration 004):
      // - chunk_text (not content)
      // - chunk_tokens (not token_count)
      // - user_id (for multi-tenant security)
      // - metadata (for debugging/traceability)
      const params: SqlParams = {
        id: chunkId,
        file_id: fileId,
        user_id: userId,
        chunk_text: chunk.text,
        chunk_index: chunk.chunkIndex,
        chunk_tokens: chunk.tokenCount,
        metadata: chunk.metadata ? JSON.stringify(chunk.metadata) : null,
      };

      await executeQuery(
        `INSERT INTO file_chunks (id, file_id, user_id, chunk_index, chunk_text, chunk_tokens, metadata, created_at)
         VALUES (@id, @file_id, @user_id, @chunk_index, @chunk_text, @chunk_tokens, @metadata, GETUTCDATE())`,
        params
      );

      chunkRecords.push({
        id: chunkId,
        text: chunk.text,
        chunkIndex: chunk.chunkIndex,
        tokenCount: chunk.tokenCount,
      });
    }

    logger.info({ fileId, insertedCount: chunkRecords.length }, 'Inserted chunks into database');

    return chunkRecords;
  }

  /**
   * Enqueue embedding generation job
   *
   * OPTIMIZED: Only passes chunk IDs to reduce Redis memory usage.
   * The worker reads chunk text from the database when processing.
   *
   * @param fileId - File ID
   * @param userId - User ID
   * @param chunks - Chunk records with IDs
   * @returns Job ID
   */
  private async enqueueEmbeddingJob(
    fileId: string,
    userId: string,
    chunks: Array<{ id: string; text: string; chunkIndex: number; tokenCount: number }>,
    mimeType: string
  ): Promise<string> {
    // Dynamic import to avoid circular dependencies
    const { getMessageQueue } = await import('@/infrastructure/queue/MessageQueue');
    const messageQueue = getMessageQueue();

    // OPTIMIZATION: Only store chunk IDs, not text content
    // This reduces Redis memory by ~80% for large file batches
    // Worker reads text from database when processing
    const jobData: EmbeddingGenerationJob = {
      fileId,
      userId,
      chunkIds: chunks.map(chunk => chunk.id),
      mimeType,
    };

    const jobId = await messageQueue.addEmbeddingGenerationJob(jobData);

    logger.info({ fileId, jobId, chunkCount: chunks.length }, 'Enqueued embedding generation job (optimized: IDs only)');

    return jobId;
  }
}

/**
 * Get FileChunkingService singleton
 */
export function getFileChunkingService(): FileChunkingService {
  return FileChunkingService.getInstance();
}
